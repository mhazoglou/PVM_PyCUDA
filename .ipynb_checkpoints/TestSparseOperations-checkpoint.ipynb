{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of nonzero elements:', 4000000)\n",
      "On CPU: 0.00412011146545 seconds\n",
      "On GPU: 0.000292062759399 seconds\n",
      "On GPU second prepared call: 9.70363616943e-05 seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Matrix A (GPU):\n",
      "[ 0.22381526  0.58321306  0.82735869 ...,  0.88366254  0.9471917\n",
      "  0.77406551]\n",
      "--------------------------------------------------------------------------------\n",
      "Vector V (GPU):\n",
      "[[ 0.36687196]\n",
      " [ 0.44676566]\n",
      " [ 1.61404282]\n",
      " ..., \n",
      " [-1.18794056]\n",
      " [-0.47910979]\n",
      " [-0.77500554]]\n",
      "--------------------------------------------------------------------------------\n",
      "Vector R (GPU):\n",
      "[[-4.36695776]\n",
      " [ 1.16036724]\n",
      " [ 1.08176592]\n",
      " ..., \n",
      " [-0.43955478]\n",
      " [-1.74111527]\n",
      " [ 0.09272674]]\n",
      "--------------------------------------------------------------------------------\n",
      "Vector R (CPU):\n",
      "[[-4.36695776]\n",
      " [ 1.16036724]\n",
      " [ 1.08176592]\n",
      " ..., \n",
      " [-0.43955478]\n",
      " [-1.74111527]\n",
      " [ 0.09272674]]\n",
      "--------------------------------------------------------------------------------\n",
      "CPU-GPU difference:\n",
      "[[  0.00000000e+00]\n",
      " [  0.00000000e+00]\n",
      " [ -2.22044605e-16]\n",
      " ..., \n",
      " [ -2.22044605e-16]\n",
      " [ -2.22044605e-16]\n",
      " [ -3.46944695e-16]]\n",
      "L2 norm: 1.80735308986e-13\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Multiples two square matrices together using multiple blocks and shared memory. \n",
    "Each thread block is assigned a \"tile\" of the resulting matrix and is responsible\n",
    "for generating the elements in that tile.  Each thread in a block computes one element \n",
    "of the tile.\n",
    "\"\"\"\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "from numpy import linalg as la\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '0'\n",
    "# -- initialize the device\n",
    "import pycuda.autoinit\n",
    "\n",
    "TestTranspose = False # tests the tranpose matrix product with a vector\n",
    "\n",
    "filename = '/home/mhazoglou/CUDA_Kernels/Useful_Kernels.cu'\n",
    "\n",
    "with open(filename) as fid:\n",
    "    kernel_code = fid.read()\n",
    "\n",
    "ceil = lambda x: int(math.ceil(x))\n",
    "\n",
    "# define length of array\n",
    "L_col = np.int32(2000)#32*32*10)#100000)#1024*1000000)\n",
    "L_row = np.int32(200000)#1024*1+1)#13*13*13*13)\n",
    "\n",
    "\n",
    "# create random arrays\n",
    "a_cpu = sp.rand(L_row, L_col, density = 0.01, format = 'csr', dtype = np.float64)\n",
    "nnz = a_cpu.nnz\n",
    "print(\"Number of nonzero elements:\", nnz)\n",
    "if TestTranspose:\n",
    "    v_cpu = np.random.randn(L_row, 1).astype(np.float64)\n",
    "else:\n",
    "    v_cpu = np.random.randn(L_col, 1).astype(np.float64)\n",
    "\n",
    "\n",
    "# compute reference on the CPU to verify GPU computation\n",
    "if TestTranspose:\n",
    "    t0 = time.time()\n",
    "    r_cpu = (a_cpu.transpose()).dot(v_cpu)\n",
    "    print 'On CPU: {} seconds'.format(time.time()-t0)\n",
    "else:\n",
    "    t0 = time.time()\n",
    "    r_cpu = a_cpu.dot(v_cpu)\n",
    "    print 'On CPU: {} seconds'.format(time.time()-t0)\n",
    "\n",
    "# transfer host (CPU) memory to device (GPU) memory \n",
    "idx = gpuarray.to_gpu(a_cpu.indices)\n",
    "ptr = gpuarray.to_gpu(a_cpu.indptr)\n",
    "a_gpu = gpuarray.to_gpu(a_cpu.data)\n",
    "v_gpu = gpuarray.to_gpu(v_cpu)\n",
    "\n",
    "# create zero initialized gpu array for the result (R = A * V)\n",
    "if TestTranspose:\n",
    "    r_init = np.zeros((L_col, 1), dtype = np.float64)\n",
    "else:\n",
    "    r_init = np.zeros((L_row, 1), dtype = np.float64)\n",
    "r_gpu = gpuarray.to_gpu(r_init)\n",
    "\n",
    "# compile the kernel code\n",
    "mod = compiler.SourceModule(kernel_code)\n",
    "\n",
    "# get the kernel function from the compiled module\n",
    "if TestTranspose:\n",
    "    dot = mod.get_function('spmTv_csr_kernel')\n",
    "    dot.prepare([np.int32, 'P', 'P', 'P', 'P', 'P'])\n",
    "else:\n",
    "    dot = mod.get_function('spmv_csr_kernel')\n",
    "    dot.prepare([np.int32, 'P', 'P', 'P', 'P', 'P'])\n",
    "\n",
    "# call the kernel on the card\n",
    "BLOCKSIZE = 1024\n",
    "grid_x = min(ceil(L_row / BLOCKSIZE), 1024)\n",
    "t1 = time.time()\n",
    "dot.prepared_call((grid_x, 1), (BLOCKSIZE, 1, 1),\n",
    "                  L_row,\n",
    "                  ptr.gpudata, idx.gpudata, a_gpu.gpudata,\n",
    "                  v_gpu.gpudata, r_gpu.gpudata)\n",
    "\n",
    "print 'On GPU: {} seconds'.format(time.time()-t1)\n",
    "\n",
    "r_gpu = gpuarray.to_gpu(r_init)\n",
    "t1 = time.time()\n",
    "# # reinitialized zero gpu array for the result (R = A * V)\n",
    "# #r_gpu = gpuarray.to_gpu(r_init) #gpuarray.empty((1, ceil(L/TILE_SIZE)), np.float32)\n",
    "\n",
    "dot.prepared_call((grid_x, 1), (BLOCKSIZE, 1, 1),\n",
    "                  L_row,\n",
    "                  ptr.gpudata, idx.gpudata, a_gpu.gpudata,\n",
    "                  v_gpu.gpudata, r_gpu.gpudata)\n",
    "print 'On GPU second prepared call: {} seconds'.format(time.time()-t1)\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Matrix A (GPU):\"\n",
    "print a_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector V (GPU):\"\n",
    "print v_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector R (GPU):\"\n",
    "print r_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector R (CPU):\"\n",
    "print r_cpu\n",
    "\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"CPU-GPU difference:\"\n",
    "print r_cpu - r_gpu.get()\n",
    "print \"L2 norm:\", la.norm(r_cpu - r_gpu.get())\n",
    "print np.allclose(r_cpu, r_gpu.get())\n",
    "\n",
    "driver.stop_profiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSR On GPU: 5.69295883179e-06 seconds\n",
      "COO On GPU 0.000169404029846 seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Vector Vx (GPU):\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]]\n",
      "--------------------------------------------------------------------------------\n",
      "Vector Vy (GPU):\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "--------------------------------------------------------------------------------\n",
      "Sparse Matrix A (GPU-CSR):\n",
      "[ 1.  1.  1. ...,  2.  2.  2.]\n",
      "--------------------------------------------------------------------------------\n",
      "Sparse Matrix A (GPU-COO)\n",
      "[ 1.  1.  1. ...,  2.  2.  2.]\n",
      "--------------------------------------------------------------------------------\n",
      "Sparse Matrix A (CPU):\n",
      "[ True  True  True ...,  True  True  True]\n",
      "--------------------------------------------------------------------------------\n",
      "CPU-GPU difference:\n",
      "[ 0.  0.  0. ..., -1. -1. -1.] [ 0.  0.  0. ..., -1. -1. -1.]\n",
      "total: -45163.0\n",
      "total: -45163.0\n",
      "GPU-GPU difference\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "total: 0.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Multiples two square matrices together using multiple blocks and shared memory. \n",
    "Each thread block is assigned a \"tile\" of the resulting matrix and is responsible\n",
    "for generating the elements in that tile.  Each thread in a block computes one element \n",
    "of the tile.\n",
    "\"\"\"\n",
    "import timeit\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "from numpy import linalg as la\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "from WeightsAndBiasInitialization import ptr_to_row\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '1'\n",
    "# -- initialize the device\n",
    "import pycuda.autoinit\n",
    "\n",
    "\n",
    "\n",
    "filename = '/home/mhazoglou/CUDA_Kernels/Useful_Kernels.cu'\n",
    "\n",
    "with open(filename) as fid:\n",
    "    kernel_code = fid.read()\n",
    "\n",
    "ceil = lambda x: int(math.ceil(x))\n",
    "\n",
    "# define length of array\n",
    "L_col = np.int32(1000)#100000)#1024*1000000)\n",
    "L_row = np.int32(1000)#1024*1+1)#13*13*13*13)\n",
    "\n",
    "\n",
    "# create random arrays\n",
    "a_cpu = sp.rand(L_row, L_col, density = 0.09, format = 'csr')\n",
    "\n",
    "vx_cpu = np.concatenate((np.ones((int(L_col/2), 1), dtype = np.float64),\n",
    "                         2*np.ones((int(L_col/2), 1), dtype = np.float64)),\n",
    "                        axis = 0)\n",
    "# np.ones((L_col, 1), dtype = np.float64)\n",
    "\n",
    "vy_cpu = np.concatenate((np.ones((int(L_row/2), 1), dtype = np.float64),\n",
    "                         np.ones((int(L_row/2), 1), dtype = np.float64)),\n",
    "                        axis = 0)\n",
    "# np.ones((L_row, 1), dtype = np.float64)\n",
    "\n",
    "row_idx = gpuarray.to_gpu(ptr_to_row(a_cpu.indptr))\n",
    "\n",
    "# transfer host (CPU) memory to device (GPU) memory \n",
    "idx = gpuarray.to_gpu(a_cpu.indices)\n",
    "ptr = gpuarray.to_gpu(a_cpu.indptr)\n",
    "vx_gpu = gpuarray.to_gpu(vx_cpu)\n",
    "vy_gpu = gpuarray.to_gpu(vy_cpu)\n",
    "\n",
    "nnz = np.int32(a_cpu.nnz)\n",
    "a_gpu = gpuarray.empty((nnz), dtype = np.float64)\n",
    "b_gpu = gpuarray.empty((nnz), dtype = np.float64)\n",
    "\n",
    "# compile the kernel code\n",
    "mod = compiler.SourceModule(kernel_code)\n",
    "\n",
    "# get the kernel function from the compiled module\n",
    "out = mod.get_function('spvv_csr_outer_kernel')\n",
    "out.prepare([np.int32, 'P', 'P', 'P', 'P', 'P'])\n",
    "\n",
    "out_coo = mod.get_function('spvv_coo_outer_kernel')\n",
    "out_coo.prepare([np.int32, 'P', 'P', 'P', 'P', 'P'])\n",
    "\n",
    "repeat = 1000\n",
    "\n",
    "# call the kernel on the card\n",
    "BLOCKSIZE = 1024\n",
    "grid_x = min(ceil(L_row / BLOCKSIZE), 1024)\n",
    "t1 = timeit.default_timer()\n",
    "for _ in range(repeat):\n",
    "    out.prepared_call((grid_x, 1), (BLOCKSIZE, 1, 1),\n",
    "                      L_row,\n",
    "                      ptr.gpudata, idx.gpudata, vx_gpu.gpudata,\n",
    "                      vy_gpu.gpudata, a_gpu.gpudata)\n",
    "\n",
    "print 'CSR On GPU: {} seconds'.format((timeit.default_timer()-t1) / repeat)\n",
    "\n",
    "\n",
    "grid_x_coo = min(ceil(nnz / BLOCKSIZE), 1024)\n",
    "t1 = timeit.default_timer()\n",
    "for _ in range(repeat):\n",
    "    out_coo.prepared_call((grid_x_coo, 1), (BLOCKSIZE, 1, 1),\n",
    "                          nnz,\n",
    "                          row_idx.gpudata, idx.gpudata,\n",
    "                          vx_gpu.gpudata, vy_gpu.gpudata, b_gpu.gpudata)\n",
    "print 'COO On GPU {} seconds'.format((timeit.default_timer() - t1) / repeat)\n",
    "\n",
    "# t1 = time.time()\n",
    "# # reinitialized zero gpu array for the result (R = A * V)\n",
    "# #r_gpu = gpuarray.to_gpu(r_init) #gpuarray.empty((1, ceil(L/TILE_SIZE)), np.float32)\n",
    "\n",
    "# r_gpu = gpuarray.to_gpu(r_init)\n",
    "# dot.prepared_call((grid_x, 1), (BLOCKSIZE, 1, 1),\n",
    "#                   L_row,\n",
    "#                   ptr.gpudata, idx.gpudata, a_gpu.gpudata,\n",
    "#                   v_gpu.gpudata, r_gpu.gpudata)\n",
    "# print 'On GPU second prepared call: {} seconds'.format(time.time()-t1)\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector Vx (GPU):\"\n",
    "print vx_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector Vy (GPU):\"\n",
    "print vy_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Sparse Matrix A (GPU-CSR):\"\n",
    "print a_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Sparse Matrix A (GPU-COO)\"\n",
    "print b_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Sparse Matrix A (CPU):\"\n",
    "print a_cpu.data.astype(np.bool)\n",
    "\n",
    "\n",
    "dif = a_cpu.data.astype(np.bool) - a_gpu.get()\n",
    "dif2 = a_cpu.data.astype(np.bool) - b_gpu.get()\n",
    "print \"-\" * 80\n",
    "print \"CPU-GPU difference:\"\n",
    "print dif, dif2\n",
    "print 'total:', sum(dif)\n",
    "print 'total:', sum(dif2)\n",
    "\n",
    "dif_gpu = a_gpu.get() - b_gpu.get()\n",
    "print \"GPU-GPU difference\"\n",
    "print dif_gpu\n",
    "print 'total:', sum(dif_gpu)\n",
    "\n",
    "driver.stop_profiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_code = \"\"\"\n",
    "\n",
    "__inline__ __device__\n",
    "double warpReduceSum(double val) {\n",
    "  for (unsigned int offset = warpSize/2; offset > 0; offset /= 2) \n",
    "    val += __shfl_down(val, offset);\n",
    "  return val;\n",
    "}\n",
    "\n",
    "__inline__ __device__\n",
    "double blockReduceSum(double val) {\n",
    "\n",
    "  static __shared__ double shared[32]; // Shared mem for 32 partial sums\n",
    "  unsigned int lane = threadIdx.x % warpSize;\n",
    "  unsigned int wid = threadIdx.x / warpSize;\n",
    "\n",
    "  val = warpReduceSum(val);     // Each warp performs partial reduction\n",
    "\n",
    "  if (lane==0) shared[wid]=val; // Write reduced value to shared memory\n",
    "\n",
    "  __syncthreads();              // Wait for all partial reductions\n",
    "\n",
    "  //read from shared memory only if that warp existed\n",
    "  val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.;\n",
    "\n",
    "  if (wid==0) val = warpReduceSum(val); //Final reduce within first warp\n",
    "\n",
    "  return val;\n",
    "}\n",
    "\n",
    "__global__ void \n",
    "Blocked1dCSRMatVecDotKernel(const unsigned int num_rows,\n",
    "                const unsigned int  *ptr,\n",
    "                const unsigned int  *indices,\n",
    "                const double *data,\n",
    "                const double *x,\n",
    "                double *y)\n",
    "{\n",
    "    \n",
    "    unsigned int  start_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    unsigned int stride_x = blockDim.x * gridDim.x;\n",
    "    unsigned int  start_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int stride_y = blockDim.y * gridDim.y;\n",
    "    \n",
    "    for (unsigned int row = start_y; row < num_rows; row += stride_y)\n",
    "    {\n",
    "        double sum = 0.;\n",
    "        unsigned int row_start = ptr[row];\n",
    "        //unsigned int row_end   = ptr[row +1];\n",
    "        unsigned int row_length = ptr[row +1] - row_start;\n",
    "        for (unsigned int j = start_x; j < row_length; j += stride_x)\n",
    "        {\n",
    "            sum += data[row_start + j] * x[indices[row_start + j]];\n",
    "        }\n",
    "        \n",
    "        sum = blockReduceSum(sum);\n",
    "        if (threadIdx.x==0)\n",
    "        {\n",
    "            atomicAdd(&y[row], sum);\n",
    "        }\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void \n",
    "BlockedCSRMatVecDotKernel(const unsigned int num_rows,\n",
    "                const unsigned int  *ptr,\n",
    "                const unsigned int  *indices,\n",
    "                const double *data,\n",
    "                const double *x,\n",
    "                double *y)\n",
    "{\n",
    "    \n",
    "    unsigned int  start_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    unsigned int stride_x = blockDim.x * gridDim.x;\n",
    "    unsigned int  start_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int stride_y = blockDim.y * gridDim.y;\n",
    "    \n",
    "    for (unsigned int row = start_y; row < num_rows; row += stride_y)\n",
    "    {\n",
    "        double sum = 0.;\n",
    "        unsigned int row_start = ptr[row];\n",
    "        //unsigned int row_end   = ptr[row +1];\n",
    "        unsigned int row_length = ptr[row +1] - row_start;\n",
    "        for (unsigned int j = start_x; j < row_length; j += stride_x)\n",
    "        {\n",
    "            sum += data[row_start + j] * x[indices[row_start + j]];\n",
    "        }\n",
    "        \n",
    "        sum = warpReduceSum(sum);\n",
    "        if (threadIdx.x==0)\n",
    "        {\n",
    "            atomicAdd(&y[row], sum);\n",
    "        }\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "/*\n",
    "// product of a matrix with a vector\n",
    "// R(L_R, 1) = A(L_R, L_V) V(L_V, 1)\n",
    "// the result R needs to be initialized with zeros (use ZeroFillKernel) \n",
    "__global__ void BlockedCSRMatVecDotKernel(double *A, double *V, double *R, \n",
    "        unsigned int *new_idx_V_arr, unsigned int *new_idx_R_arr,\n",
    "        unsigned int *new_block_arr, unsigned int N_blocks)\n",
    "{\n",
    "    unsigned int starting_block = threadIdx.z + blockIdx.z * blockDim.z;\n",
    "    unsigned int stride_z = blockDim.z * gridDim.z;\n",
    "    unsigned int starting_col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    unsigned int stride_x = blockDim.x * gridDim.x;\n",
    "    unsigned int starting_row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    unsigned int stride_y = blockDim.y * gridDim.y;\n",
    "    for (unsigned int block = starting_block;\n",
    "         block < N_blocks;\n",
    "         block += stride_z)\n",
    "    {\n",
    "        double sum = 0.;\n",
    "        unsigned int idx_block_start = new_block_arr[block];\n",
    "        unsigned int idx_V_start = new_idx_V_arr[block];\n",
    "        unsigned int idx_V_end = new_idx_V_arr[block + 1];\n",
    "        unsigned int idx_R_start = new_idx_R_arr[block];\n",
    "        unsigned int idx_R_end = new_idx_R_arr[block + 1];\n",
    "        \n",
    "        unsigned int L_V = idx_V_end - idx_V_start;\n",
    "        unsigned int L_R = idx_R_end - idx_R_start;\n",
    "        for (unsigned int row = starting_row;\n",
    "             row < L_R;\n",
    "             row += stride_y) \n",
    "        {\n",
    "            //reduce multiple elements per thread\n",
    "            for (unsigned int col = starting_col; \n",
    "                 col < L_V; \n",
    "                 col += stride_x) {\n",
    "                //sum += A[idx_block_start + col + row * L_V] * V[idx_V_start + col];\n",
    "                sum += A[col + row * L_V] * V[col];\n",
    "\n",
    "            }\n",
    "\n",
    "            sum = warpReduceSum(sum);\n",
    "            //static __shared__ double shared[32]; // Shared mem for 32 partial sums\n",
    "            //shared[threadIdx.y] = sum;\n",
    "\n",
    "\n",
    "            if (threadIdx.x==0)\n",
    "            {\n",
    "                //atomicAdd(&R[idx_R_start + row], sum);//shared[threadIdx.y]);\n",
    "                atomicAdd(&R[row], sum);//shared[threadIdx.y]);\n",
    "\n",
    "            }\n",
    "\n",
    "        }\n",
    "    }\n",
    "}\n",
    "*/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def looped_blockedMatVecProd(A, V, R, new_idx_V, new_idx_R, new_block, N_blocks):\n",
    "    \n",
    "    for z in xrange(N_blocks):\n",
    "        idx_block_start = new_block[z]\n",
    "        idx_V_start = new_idx_V[z]\n",
    "        idx_V_end = new_idx_V[z + 1]\n",
    "        idx_R_start = new_idx_R[z]\n",
    "        idx_R_end = new_idx_R[z + 1]\n",
    "        \n",
    "        L_V = idx_V_end - idx_V_start\n",
    "        L_R = idx_R_end - idx_R_start\n",
    "        for row in xrange(L_R):\n",
    "            for col in xrange(L_V):\n",
    "                R[idx_R_start + row] += A[idx_block_start + col + row * L_V] * V[idx_V_start + col]\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from RectangularGridConstructor import make_connections\n",
    "from WeightsAndBiasInitialization import weight_initialize\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "structure = [16, 8, 4, 3, 2, 1]\n",
    "output_sizes = [1, 2*2, 4*4, 8*8, 6*6, 16*16]\n",
    "input_size = 6 * 6 * 3\n",
    "hidden_size = 49\n",
    "connect_dict = make_connections(structure, output_sizes, context_from_top_0_0=False)\n",
    "i2h, h2op, maps, input_new_unit, op_new_unit = weight_initialize(connect_dict, input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_blocks = np.int32(len(connect_dict))\n",
    "hid_new_unit = range(0, hidden_size * N_blocks  + 1, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_block_idx(new_idx_V_arr, new_idx_R_arr):\n",
    "    block_idx_list = []\n",
    "    prev_idx_V, prev_idx_R = 0, 0\n",
    "    for idx_V, idx_R in zip(new_idx_V_arr, new_idx_R_arr):\n",
    "        L_V = idx_V-prev_idx_V\n",
    "        L_R = idx_R-prev_idx_R\n",
    "        if block_idx_list != []:\n",
    "            block_idx_list.append(block_idx_list[-1] +\n",
    "                                  L_V * L_R)\n",
    "            if L_V > max_L_V:\n",
    "                max_L_V = L_V\n",
    "            if L_R > max_L_R:\n",
    "                max_L_R = L_R\n",
    "        else:\n",
    "            block_idx_list.append(L_V * L_R)\n",
    "            max_L_V = L_V\n",
    "            max_L_R = L_R\n",
    "        prev_idx_V, prev_idx_R = idx_V, idx_R\n",
    "    return block_idx_list, max_L_V, max_L_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_block_list, max_L_V, max_L_R = new_block_idx(input_new_unit, hid_new_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 30772,\n",
       " 63945,\n",
       " 97118,\n",
       " 130291,\n",
       " 163464,\n",
       " 196637,\n",
       " 229810,\n",
       " 262983,\n",
       " 296156,\n",
       " 329329,\n",
       " 362502,\n",
       " 395675,\n",
       " 428848,\n",
       " 462021,\n",
       " 495194,\n",
       " 525966,\n",
       " 559139,\n",
       " 594713,\n",
       " 630287,\n",
       " 665861,\n",
       " 701435,\n",
       " 737009,\n",
       " 772583,\n",
       " 808157,\n",
       " 843731,\n",
       " 879305,\n",
       " 914879,\n",
       " 950453,\n",
       " 986027,\n",
       " 1021601,\n",
       " 1057175,\n",
       " 1090348,\n",
       " 1123521,\n",
       " 1159095,\n",
       " 1194669,\n",
       " 1230243,\n",
       " 1265817,\n",
       " 1301391,\n",
       " 1336965,\n",
       " 1372539,\n",
       " 1408113,\n",
       " 1443687,\n",
       " 1479261,\n",
       " 1514835,\n",
       " 1550409,\n",
       " 1585983,\n",
       " 1621557,\n",
       " 1654730,\n",
       " 1687903,\n",
       " 1723477,\n",
       " 1759051,\n",
       " 1794625,\n",
       " 1830199,\n",
       " 1865773,\n",
       " 1901347,\n",
       " 1936921,\n",
       " 1972495,\n",
       " 2008069,\n",
       " 2043643,\n",
       " 2079217,\n",
       " 2114791,\n",
       " 2150365,\n",
       " 2185939,\n",
       " 2219112,\n",
       " 2252285,\n",
       " 2287859,\n",
       " 2323433,\n",
       " 2359007,\n",
       " 2394581,\n",
       " 2430155,\n",
       " 2465729,\n",
       " 2501303,\n",
       " 2536877,\n",
       " 2572451,\n",
       " 2608025,\n",
       " 2643599,\n",
       " 2679173,\n",
       " 2714747,\n",
       " 2750321,\n",
       " 2783494,\n",
       " 2816667,\n",
       " 2852241,\n",
       " 2887815,\n",
       " 2923389,\n",
       " 2958963,\n",
       " 2994537,\n",
       " 3030111,\n",
       " 3065685,\n",
       " 3101259,\n",
       " 3136833,\n",
       " 3172407,\n",
       " 3207981,\n",
       " 3243555,\n",
       " 3279129,\n",
       " 3314703,\n",
       " 3347876,\n",
       " 3381049,\n",
       " 3416623,\n",
       " 3452197,\n",
       " 3487771,\n",
       " 3523345,\n",
       " 3558919,\n",
       " 3594493,\n",
       " 3630067,\n",
       " 3665641,\n",
       " 3701215,\n",
       " 3736789,\n",
       " 3772363,\n",
       " 3807937,\n",
       " 3843511,\n",
       " 3879085,\n",
       " 3912258,\n",
       " 3945431,\n",
       " 3981005,\n",
       " 4016579,\n",
       " 4052153,\n",
       " 4087727,\n",
       " 4123301,\n",
       " 4158875,\n",
       " 4194449,\n",
       " 4230023,\n",
       " 4265597,\n",
       " 4301171,\n",
       " 4336745,\n",
       " 4372319,\n",
       " 4407893,\n",
       " 4443467,\n",
       " 4476640,\n",
       " 4509813,\n",
       " 4545387,\n",
       " 4580961,\n",
       " 4616535,\n",
       " 4652109,\n",
       " 4687683,\n",
       " 4723257,\n",
       " 4758831,\n",
       " 4794405,\n",
       " 4829979,\n",
       " 4865553,\n",
       " 4901127,\n",
       " 4936701,\n",
       " 4972275,\n",
       " 5007849,\n",
       " 5041022,\n",
       " 5074195,\n",
       " 5109769,\n",
       " 5145343,\n",
       " 5180917,\n",
       " 5216491,\n",
       " 5252065,\n",
       " 5287639,\n",
       " 5323213,\n",
       " 5358787,\n",
       " 5394361,\n",
       " 5429935,\n",
       " 5465509,\n",
       " 5501083,\n",
       " 5536657,\n",
       " 5572231,\n",
       " 5605404,\n",
       " 5638577,\n",
       " 5674151,\n",
       " 5709725,\n",
       " 5745299,\n",
       " 5780873,\n",
       " 5816447,\n",
       " 5852021,\n",
       " 5887595,\n",
       " 5923169,\n",
       " 5958743,\n",
       " 5994317,\n",
       " 6029891,\n",
       " 6065465,\n",
       " 6101039,\n",
       " 6136613,\n",
       " 6169786,\n",
       " 6202959,\n",
       " 6238533,\n",
       " 6274107,\n",
       " 6309681,\n",
       " 6345255,\n",
       " 6380829,\n",
       " 6416403,\n",
       " 6451977,\n",
       " 6487551,\n",
       " 6523125,\n",
       " 6558699,\n",
       " 6594273,\n",
       " 6629847,\n",
       " 6665421,\n",
       " 6700995,\n",
       " 6734168,\n",
       " 6767341,\n",
       " 6802915,\n",
       " 6838489,\n",
       " 6874063,\n",
       " 6909637,\n",
       " 6945211,\n",
       " 6980785,\n",
       " 7016359,\n",
       " 7051933,\n",
       " 7087507,\n",
       " 7123081,\n",
       " 7158655,\n",
       " 7194229,\n",
       " 7229803,\n",
       " 7265377,\n",
       " 7298550,\n",
       " 7331723,\n",
       " 7367297,\n",
       " 7402871,\n",
       " 7438445,\n",
       " 7474019,\n",
       " 7509593,\n",
       " 7545167,\n",
       " 7580741,\n",
       " 7616315,\n",
       " 7651889,\n",
       " 7687463,\n",
       " 7723037,\n",
       " 7758611,\n",
       " 7794185,\n",
       " 7829759,\n",
       " 7862932,\n",
       " 7896105,\n",
       " 7931679,\n",
       " 7967253,\n",
       " 8002827,\n",
       " 8038401,\n",
       " 8073975,\n",
       " 8109549,\n",
       " 8145123,\n",
       " 8180697,\n",
       " 8216271,\n",
       " 8251845,\n",
       " 8287419,\n",
       " 8322993,\n",
       " 8358567,\n",
       " 8394141,\n",
       " 8427314,\n",
       " 8458086,\n",
       " 8491259,\n",
       " 8524432,\n",
       " 8557605,\n",
       " 8590778,\n",
       " 8623951,\n",
       " 8657124,\n",
       " 8690297,\n",
       " 8723470,\n",
       " 8756643,\n",
       " 8789816,\n",
       " 8822989,\n",
       " 8856162,\n",
       " 8889335,\n",
       " 8922508,\n",
       " 8953280,\n",
       " 9001300,\n",
       " 9051721,\n",
       " 9102142,\n",
       " 9152563,\n",
       " 9202984,\n",
       " 9253405,\n",
       " 9303826,\n",
       " 9351846,\n",
       " 9402267,\n",
       " 9455089,\n",
       " 9507911,\n",
       " 9560733,\n",
       " 9613555,\n",
       " 9666377,\n",
       " 9719199,\n",
       " 9769620,\n",
       " 9820041,\n",
       " 9872863,\n",
       " 9925685,\n",
       " 9978507,\n",
       " 10031329,\n",
       " 10084151,\n",
       " 10136973,\n",
       " 10187394,\n",
       " 10237815,\n",
       " 10290637,\n",
       " 10343459,\n",
       " 10396281,\n",
       " 10449103,\n",
       " 10501925,\n",
       " 10554747,\n",
       " 10605168,\n",
       " 10655589,\n",
       " 10708411,\n",
       " 10761233,\n",
       " 10814055,\n",
       " 10866877,\n",
       " 10919699,\n",
       " 10972521,\n",
       " 11022942,\n",
       " 11073363,\n",
       " 11126185,\n",
       " 11179007,\n",
       " 11231829,\n",
       " 11284651,\n",
       " 11337473,\n",
       " 11390295,\n",
       " 11440716,\n",
       " 11491137,\n",
       " 11543959,\n",
       " 11596781,\n",
       " 11649603,\n",
       " 11702425,\n",
       " 11755247,\n",
       " 11808069,\n",
       " 11858490,\n",
       " 11906510,\n",
       " 11956931,\n",
       " 12007352,\n",
       " 12057773,\n",
       " 12108194,\n",
       " 12158615,\n",
       " 12209036,\n",
       " 12257056,\n",
       " 12305076,\n",
       " 12357898,\n",
       " 12410720,\n",
       " 12458740,\n",
       " 12511562,\n",
       " 12571587,\n",
       " 12631612,\n",
       " 12684434,\n",
       " 12737256,\n",
       " 12797281,\n",
       " 12857306,\n",
       " 12910128,\n",
       " 12958148,\n",
       " 13010970,\n",
       " 13063792,\n",
       " 13111812,\n",
       " 13159832,\n",
       " 13212654,\n",
       " 13260674,\n",
       " 13313496,\n",
       " 13373521,\n",
       " 13426343,\n",
       " 13474363,\n",
       " 13527185,\n",
       " 13575205,\n",
       " 13620824,\n",
       " 13666443,\n",
       " 13712062,\n",
       " 13757681,\n",
       " 13798498]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_block_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  628,\n",
       "  1305,\n",
       "  1982,\n",
       "  2659,\n",
       "  3336,\n",
       "  4013,\n",
       "  4690,\n",
       "  5367,\n",
       "  6044,\n",
       "  6721,\n",
       "  7398,\n",
       "  8075,\n",
       "  8752,\n",
       "  9429,\n",
       "  10106,\n",
       "  10734,\n",
       "  11411,\n",
       "  12137,\n",
       "  12863,\n",
       "  13589,\n",
       "  14315,\n",
       "  15041,\n",
       "  15767,\n",
       "  16493,\n",
       "  17219,\n",
       "  17945,\n",
       "  18671,\n",
       "  19397,\n",
       "  20123,\n",
       "  20849,\n",
       "  21575,\n",
       "  22252,\n",
       "  22929,\n",
       "  23655,\n",
       "  24381,\n",
       "  25107,\n",
       "  25833,\n",
       "  26559,\n",
       "  27285,\n",
       "  28011,\n",
       "  28737,\n",
       "  29463,\n",
       "  30189,\n",
       "  30915,\n",
       "  31641,\n",
       "  32367,\n",
       "  33093,\n",
       "  33770,\n",
       "  34447,\n",
       "  35173,\n",
       "  35899,\n",
       "  36625,\n",
       "  37351,\n",
       "  38077,\n",
       "  38803,\n",
       "  39529,\n",
       "  40255,\n",
       "  40981,\n",
       "  41707,\n",
       "  42433,\n",
       "  43159,\n",
       "  43885,\n",
       "  44611,\n",
       "  45288,\n",
       "  45965,\n",
       "  46691,\n",
       "  47417,\n",
       "  48143,\n",
       "  48869,\n",
       "  49595,\n",
       "  50321,\n",
       "  51047,\n",
       "  51773,\n",
       "  52499,\n",
       "  53225,\n",
       "  53951,\n",
       "  54677,\n",
       "  55403,\n",
       "  56129,\n",
       "  56806,\n",
       "  57483,\n",
       "  58209,\n",
       "  58935,\n",
       "  59661,\n",
       "  60387,\n",
       "  61113,\n",
       "  61839,\n",
       "  62565,\n",
       "  63291,\n",
       "  64017,\n",
       "  64743,\n",
       "  65469,\n",
       "  66195,\n",
       "  66921,\n",
       "  67647,\n",
       "  68324,\n",
       "  69001,\n",
       "  69727,\n",
       "  70453,\n",
       "  71179,\n",
       "  71905,\n",
       "  72631,\n",
       "  73357,\n",
       "  74083,\n",
       "  74809,\n",
       "  75535,\n",
       "  76261,\n",
       "  76987,\n",
       "  77713,\n",
       "  78439,\n",
       "  79165,\n",
       "  79842,\n",
       "  80519,\n",
       "  81245,\n",
       "  81971,\n",
       "  82697,\n",
       "  83423,\n",
       "  84149,\n",
       "  84875,\n",
       "  85601,\n",
       "  86327,\n",
       "  87053,\n",
       "  87779,\n",
       "  88505,\n",
       "  89231,\n",
       "  89957,\n",
       "  90683,\n",
       "  91360,\n",
       "  92037,\n",
       "  92763,\n",
       "  93489,\n",
       "  94215,\n",
       "  94941,\n",
       "  95667,\n",
       "  96393,\n",
       "  97119,\n",
       "  97845,\n",
       "  98571,\n",
       "  99297,\n",
       "  100023,\n",
       "  100749,\n",
       "  101475,\n",
       "  102201,\n",
       "  102878,\n",
       "  103555,\n",
       "  104281,\n",
       "  105007,\n",
       "  105733,\n",
       "  106459,\n",
       "  107185,\n",
       "  107911,\n",
       "  108637,\n",
       "  109363,\n",
       "  110089,\n",
       "  110815,\n",
       "  111541,\n",
       "  112267,\n",
       "  112993,\n",
       "  113719,\n",
       "  114396,\n",
       "  115073,\n",
       "  115799,\n",
       "  116525,\n",
       "  117251,\n",
       "  117977,\n",
       "  118703,\n",
       "  119429,\n",
       "  120155,\n",
       "  120881,\n",
       "  121607,\n",
       "  122333,\n",
       "  123059,\n",
       "  123785,\n",
       "  124511,\n",
       "  125237,\n",
       "  125914,\n",
       "  126591,\n",
       "  127317,\n",
       "  128043,\n",
       "  128769,\n",
       "  129495,\n",
       "  130221,\n",
       "  130947,\n",
       "  131673,\n",
       "  132399,\n",
       "  133125,\n",
       "  133851,\n",
       "  134577,\n",
       "  135303,\n",
       "  136029,\n",
       "  136755,\n",
       "  137432,\n",
       "  138109,\n",
       "  138835,\n",
       "  139561,\n",
       "  140287,\n",
       "  141013,\n",
       "  141739,\n",
       "  142465,\n",
       "  143191,\n",
       "  143917,\n",
       "  144643,\n",
       "  145369,\n",
       "  146095,\n",
       "  146821,\n",
       "  147547,\n",
       "  148273,\n",
       "  148950,\n",
       "  149627,\n",
       "  150353,\n",
       "  151079,\n",
       "  151805,\n",
       "  152531,\n",
       "  153257,\n",
       "  153983,\n",
       "  154709,\n",
       "  155435,\n",
       "  156161,\n",
       "  156887,\n",
       "  157613,\n",
       "  158339,\n",
       "  159065,\n",
       "  159791,\n",
       "  160468,\n",
       "  161145,\n",
       "  161871,\n",
       "  162597,\n",
       "  163323,\n",
       "  164049,\n",
       "  164775,\n",
       "  165501,\n",
       "  166227,\n",
       "  166953,\n",
       "  167679,\n",
       "  168405,\n",
       "  169131,\n",
       "  169857,\n",
       "  170583,\n",
       "  171309,\n",
       "  171986,\n",
       "  172614,\n",
       "  173291,\n",
       "  173968,\n",
       "  174645,\n",
       "  175322,\n",
       "  175999,\n",
       "  176676,\n",
       "  177353,\n",
       "  178030,\n",
       "  178707,\n",
       "  179384,\n",
       "  180061,\n",
       "  180738,\n",
       "  181415,\n",
       "  182092,\n",
       "  182720,\n",
       "  183700,\n",
       "  184729,\n",
       "  185758,\n",
       "  186787,\n",
       "  187816,\n",
       "  188845,\n",
       "  189874,\n",
       "  190854,\n",
       "  191883,\n",
       "  192961,\n",
       "  194039,\n",
       "  195117,\n",
       "  196195,\n",
       "  197273,\n",
       "  198351,\n",
       "  199380,\n",
       "  200409,\n",
       "  201487,\n",
       "  202565,\n",
       "  203643,\n",
       "  204721,\n",
       "  205799,\n",
       "  206877,\n",
       "  207906,\n",
       "  208935,\n",
       "  210013,\n",
       "  211091,\n",
       "  212169,\n",
       "  213247,\n",
       "  214325,\n",
       "  215403,\n",
       "  216432,\n",
       "  217461,\n",
       "  218539,\n",
       "  219617,\n",
       "  220695,\n",
       "  221773,\n",
       "  222851,\n",
       "  223929,\n",
       "  224958,\n",
       "  225987,\n",
       "  227065,\n",
       "  228143,\n",
       "  229221,\n",
       "  230299,\n",
       "  231377,\n",
       "  232455,\n",
       "  233484,\n",
       "  234513,\n",
       "  235591,\n",
       "  236669,\n",
       "  237747,\n",
       "  238825,\n",
       "  239903,\n",
       "  240981,\n",
       "  242010,\n",
       "  242990,\n",
       "  244019,\n",
       "  245048,\n",
       "  246077,\n",
       "  247106,\n",
       "  248135,\n",
       "  249164,\n",
       "  250144,\n",
       "  251124,\n",
       "  252202,\n",
       "  253280,\n",
       "  254260,\n",
       "  255338,\n",
       "  256563,\n",
       "  257788,\n",
       "  258866,\n",
       "  259944,\n",
       "  261169,\n",
       "  262394,\n",
       "  263472,\n",
       "  264452,\n",
       "  265530,\n",
       "  266608,\n",
       "  267588,\n",
       "  268568,\n",
       "  269646,\n",
       "  270626,\n",
       "  271704,\n",
       "  272929,\n",
       "  274007,\n",
       "  274987,\n",
       "  276065,\n",
       "  277045,\n",
       "  277976,\n",
       "  278907,\n",
       "  279838,\n",
       "  280769,\n",
       "  281602],\n",
       " [0,\n",
       "  109,\n",
       "  218,\n",
       "  327,\n",
       "  436,\n",
       "  545,\n",
       "  654,\n",
       "  763,\n",
       "  872,\n",
       "  981,\n",
       "  1090,\n",
       "  1199,\n",
       "  1308,\n",
       "  1417,\n",
       "  1526,\n",
       "  1635,\n",
       "  1744,\n",
       "  1853,\n",
       "  1962,\n",
       "  2071,\n",
       "  2180,\n",
       "  2289,\n",
       "  2398,\n",
       "  2507,\n",
       "  2616,\n",
       "  2725,\n",
       "  2834,\n",
       "  2943,\n",
       "  3052,\n",
       "  3161,\n",
       "  3270,\n",
       "  3379,\n",
       "  3488,\n",
       "  3597,\n",
       "  3706,\n",
       "  3815,\n",
       "  3924,\n",
       "  4033,\n",
       "  4142,\n",
       "  4251,\n",
       "  4360,\n",
       "  4469,\n",
       "  4578,\n",
       "  4687,\n",
       "  4796,\n",
       "  4905,\n",
       "  5014,\n",
       "  5123,\n",
       "  5232,\n",
       "  5341,\n",
       "  5450,\n",
       "  5559,\n",
       "  5668,\n",
       "  5777,\n",
       "  5886,\n",
       "  5995,\n",
       "  6104,\n",
       "  6213,\n",
       "  6322,\n",
       "  6431,\n",
       "  6540,\n",
       "  6649,\n",
       "  6758,\n",
       "  6867,\n",
       "  6976,\n",
       "  7085,\n",
       "  7194,\n",
       "  7303,\n",
       "  7412,\n",
       "  7521,\n",
       "  7630,\n",
       "  7739,\n",
       "  7848,\n",
       "  7957,\n",
       "  8066,\n",
       "  8175,\n",
       "  8284,\n",
       "  8393,\n",
       "  8502,\n",
       "  8611,\n",
       "  8720,\n",
       "  8829,\n",
       "  8938,\n",
       "  9047,\n",
       "  9156,\n",
       "  9265,\n",
       "  9374,\n",
       "  9483,\n",
       "  9592,\n",
       "  9701,\n",
       "  9810,\n",
       "  9919,\n",
       "  10028,\n",
       "  10137,\n",
       "  10246,\n",
       "  10355,\n",
       "  10464,\n",
       "  10573,\n",
       "  10682,\n",
       "  10791,\n",
       "  10900,\n",
       "  11009,\n",
       "  11118,\n",
       "  11227,\n",
       "  11336,\n",
       "  11445,\n",
       "  11554,\n",
       "  11663,\n",
       "  11772,\n",
       "  11881,\n",
       "  11990,\n",
       "  12099,\n",
       "  12208,\n",
       "  12317,\n",
       "  12426,\n",
       "  12535,\n",
       "  12644,\n",
       "  12753,\n",
       "  12862,\n",
       "  12971,\n",
       "  13080,\n",
       "  13189,\n",
       "  13298,\n",
       "  13407,\n",
       "  13516,\n",
       "  13625,\n",
       "  13734,\n",
       "  13843,\n",
       "  13952,\n",
       "  14061,\n",
       "  14170,\n",
       "  14279,\n",
       "  14388,\n",
       "  14497,\n",
       "  14606,\n",
       "  14715,\n",
       "  14824,\n",
       "  14933,\n",
       "  15042,\n",
       "  15151,\n",
       "  15260,\n",
       "  15369,\n",
       "  15478,\n",
       "  15587,\n",
       "  15696,\n",
       "  15805,\n",
       "  15914,\n",
       "  16023,\n",
       "  16132,\n",
       "  16241,\n",
       "  16350,\n",
       "  16459,\n",
       "  16568,\n",
       "  16677,\n",
       "  16786,\n",
       "  16895,\n",
       "  17004,\n",
       "  17113,\n",
       "  17222,\n",
       "  17331,\n",
       "  17440,\n",
       "  17549,\n",
       "  17658,\n",
       "  17767,\n",
       "  17876,\n",
       "  17985,\n",
       "  18094,\n",
       "  18203,\n",
       "  18312,\n",
       "  18421,\n",
       "  18530,\n",
       "  18639,\n",
       "  18748,\n",
       "  18857,\n",
       "  18966,\n",
       "  19075,\n",
       "  19184,\n",
       "  19293,\n",
       "  19402,\n",
       "  19511,\n",
       "  19620,\n",
       "  19729,\n",
       "  19838,\n",
       "  19947,\n",
       "  20056,\n",
       "  20165,\n",
       "  20274,\n",
       "  20383,\n",
       "  20492,\n",
       "  20601,\n",
       "  20710,\n",
       "  20819,\n",
       "  20928,\n",
       "  21037,\n",
       "  21146,\n",
       "  21255,\n",
       "  21364,\n",
       "  21473,\n",
       "  21582,\n",
       "  21691,\n",
       "  21800,\n",
       "  21909,\n",
       "  22018,\n",
       "  22127,\n",
       "  22236,\n",
       "  22345,\n",
       "  22454,\n",
       "  22563,\n",
       "  22672,\n",
       "  22781,\n",
       "  22890,\n",
       "  22999,\n",
       "  23108,\n",
       "  23217,\n",
       "  23326,\n",
       "  23435,\n",
       "  23544,\n",
       "  23653,\n",
       "  23762,\n",
       "  23871,\n",
       "  23980,\n",
       "  24089,\n",
       "  24198,\n",
       "  24307,\n",
       "  24416,\n",
       "  24525,\n",
       "  24634,\n",
       "  24743,\n",
       "  24852,\n",
       "  24961,\n",
       "  25070,\n",
       "  25179,\n",
       "  25288,\n",
       "  25397,\n",
       "  25506,\n",
       "  25615,\n",
       "  25724,\n",
       "  25833,\n",
       "  25942,\n",
       "  26051,\n",
       "  26160,\n",
       "  26269,\n",
       "  26378,\n",
       "  26487,\n",
       "  26596,\n",
       "  26705,\n",
       "  26814,\n",
       "  26923,\n",
       "  27032,\n",
       "  27141,\n",
       "  27250,\n",
       "  27359,\n",
       "  27468,\n",
       "  27577,\n",
       "  27686,\n",
       "  27795,\n",
       "  27904,\n",
       "  28104,\n",
       "  28304,\n",
       "  28504,\n",
       "  28704,\n",
       "  28904,\n",
       "  29104,\n",
       "  29304,\n",
       "  29504,\n",
       "  29704,\n",
       "  29904,\n",
       "  30104,\n",
       "  30304,\n",
       "  30504,\n",
       "  30704,\n",
       "  30904,\n",
       "  31104,\n",
       "  31304,\n",
       "  31504,\n",
       "  31704,\n",
       "  31904,\n",
       "  32104,\n",
       "  32304,\n",
       "  32504,\n",
       "  32704,\n",
       "  32904,\n",
       "  33104,\n",
       "  33304,\n",
       "  33504,\n",
       "  33704,\n",
       "  33904,\n",
       "  34104,\n",
       "  34304,\n",
       "  34504,\n",
       "  34704,\n",
       "  34904,\n",
       "  35104,\n",
       "  35304,\n",
       "  35504,\n",
       "  35704,\n",
       "  35904,\n",
       "  36104,\n",
       "  36304,\n",
       "  36504,\n",
       "  36704,\n",
       "  36904,\n",
       "  37104,\n",
       "  37304,\n",
       "  37504,\n",
       "  37704,\n",
       "  37904,\n",
       "  38104,\n",
       "  38304,\n",
       "  38504,\n",
       "  38704,\n",
       "  38904,\n",
       "  39104,\n",
       "  39304,\n",
       "  39504,\n",
       "  39704,\n",
       "  39904,\n",
       "  40104,\n",
       "  40304,\n",
       "  40504,\n",
       "  40704,\n",
       "  40916,\n",
       "  41128,\n",
       "  41340,\n",
       "  41552,\n",
       "  41764,\n",
       "  41976,\n",
       "  42188,\n",
       "  42400,\n",
       "  42612,\n",
       "  42824,\n",
       "  43036,\n",
       "  43248,\n",
       "  43460,\n",
       "  43672,\n",
       "  43884,\n",
       "  44096,\n",
       "  44356,\n",
       "  44616,\n",
       "  44876,\n",
       "  45136,\n",
       "  45396,\n",
       "  45656,\n",
       "  45916,\n",
       "  46176,\n",
       "  46436,\n",
       "  46668,\n",
       "  46900,\n",
       "  47132,\n",
       "  47364,\n",
       "  47816])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_new_unit, op_new_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Multiples two square matrices together using multiple blocks and shared memory. \n",
    "Each thread block is assigned a \"tile\" of the resulting matrix and is responsible\n",
    "for generating the elements in that tile.  Each thread in a block computes one element \n",
    "of the tile.\n",
    "\"\"\"\n",
    "import time\n",
    "import math\n",
    "from scipy import sparse as sp\n",
    "from numpy import linalg as la\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '0'\n",
    "# -- initialize the device\n",
    "import pycuda.autoinit\n",
    "\n",
    "TestTranspose = False # tests the tranpose matrix product with a vector\n",
    "\n",
    "ceil = lambda x: int(math.ceil(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define length of array\n",
    "# L_col = np.int32(2000)#32*32*10)#100000)#1024*1000000)\n",
    "# L_row = np.int32(200000)#1024*1+1)#13*13*13*13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of nonzero elements:', 13798498)\n",
      "On CPU: 0.0128030776978 seconds\n"
     ]
    }
   ],
   "source": [
    "# create random arrays\n",
    "a_cpu = sp.csr_matrix((i2h[2], i2h[1], i2h[0]), dtype=np.float64)\n",
    "L_row, L_col = np.int32(a_cpu.shape)\n",
    "nnz = a_cpu.nnz\n",
    "print(\"Number of nonzero elements:\", nnz)\n",
    "if TestTranspose:\n",
    "    v_cpu = np.random.randn(L_row, 1).astype(np.float64)\n",
    "else:\n",
    "    v_cpu = np.random.randn(input_new_unit[-1], 1).astype(np.float64)\n",
    "\n",
    "\n",
    "# compute reference on the CPU to verify GPU computation\n",
    "if TestTranspose:\n",
    "    t0 = time.time()\n",
    "    r_cpu = (a_cpu.transpose()).dot(v_cpu)\n",
    "    print 'On CPU: {} seconds'.format(time.time()-t0)\n",
    "else:\n",
    "    t0 = time.time()\n",
    "    r_cpu = a_cpu.dot(v_cpu)\n",
    "    print 'On CPU: {} seconds'.format(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.63989466]\n",
      " [-0.71248572]\n",
      " [ 0.50008168]\n",
      " ..., \n",
      " [-0.50035529]\n",
      " [ 0.34804616]\n",
      " [ 0.08704561]]\n"
     ]
    }
   ],
   "source": [
    "print r_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transfer host (CPU) memory to device (GPU) memory \n",
    "# new_idx_V = gpuarray.to_gpu(np.array(input_new_unit, dtype=np.uint))\n",
    "# new_idx_R = gpuarray.to_gpu(np.array(hid_new_unit, dtype=np.uint))\n",
    "# new_block_arr = gpuarray.to_gpu(np.array(new_block_list, dtype=np.uint))\n",
    "idx = gpuarray.to_gpu(a_cpu.indices)\n",
    "ptr = gpuarray.to_gpu(a_cpu.indptr)\n",
    "a_gpu = gpuarray.to_gpu(a_cpu.data)\n",
    "v_gpu = gpuarray.to_gpu(v_cpu)\n",
    "\n",
    "# create zero initialized gpu array for the result (R = A * V)\n",
    "if TestTranspose:\n",
    "    r_init = np.zeros((L_col, 1), dtype = np.float64)\n",
    "else:\n",
    "    r_init = np.zeros((hid_new_unit[-1], 1), dtype = np.float64)\n",
    "r_gpu = gpuarray.to_gpu(r_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        ..., \n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]),\n",
       " array([ 0.03390897, -0.01061022, -0.02943279, ...,  0.01206078,\n",
       "         0.02631849, -0.02495353]),\n",
       " array([[ 0.81108997],\n",
       "        [-0.75824377],\n",
       "        [-0.48941608],\n",
       "        ..., \n",
       "        [ 0.80885355],\n",
       "        [-0.75099613],\n",
       "        [-1.26248197]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_gpu.get(), a_gpu.get(), v_gpu.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(new_idx_V.get()), len(new_idx_R.get()), len(new_block_arr.get()), N_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On GPU: 0.000101089477539 seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Matrix A (GPU):\n",
      "[ 0.03390897 -0.01061022 -0.02943279 ...,  0.01206078  0.02631849\n",
      " -0.02495353]\n",
      "--------------------------------------------------------------------------------\n",
      "Vector V (GPU):\n",
      "[[ 0.81108997]\n",
      " [-0.75824377]\n",
      " [-0.48941608]\n",
      " ..., \n",
      " [ 0.80885355]\n",
      " [-0.75099613]\n",
      " [-1.26248197]]\n",
      "--------------------------------------------------------------------------------\n",
      "Vector R (GPU):\n",
      "[[-2.55957864]\n",
      " [-2.84994286]\n",
      " [ 2.00032671]\n",
      " ..., \n",
      " [-2.00142115]\n",
      " [ 1.39218466]\n",
      " [ 0.34818243]]\n",
      "--------------------------------------------------------------------------------\n",
      "Vector R (CPU):\n",
      "[[-0.63989466]\n",
      " [-0.71248572]\n",
      " [ 0.50008168]\n",
      " ..., \n",
      " [-0.50035529]\n",
      " [ 0.34804616]\n",
      " [ 0.08704561]]\n",
      "--------------------------------------------------------------------------------\n",
      "CPU-GPU difference:\n",
      "[[ 1.91968398]\n",
      " [ 2.13745715]\n",
      " [-1.50024503]\n",
      " ..., \n",
      " [ 1.50106587]\n",
      " [-1.04413849]\n",
      " [-0.26113682]]\n",
      "L2 norm: 393.413367972\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# compile the kernel code\n",
    "mod = compiler.SourceModule(kernel_code)\n",
    "\n",
    "# get the kernel function from the compiled module\n",
    "if TestTranspose:\n",
    "    dot = mod.get_function('BlockedCSRMatVecDotKernel')\n",
    "    dot.prepare([np.int32, 'P', 'P', 'P', 'P', 'P'])\n",
    "else:\n",
    "    dot = mod.get_function('Blocked1dCSRMatVecDotKernel')\n",
    "    dot.prepare([np.int32, 'P', 'P', 'P', 'P', 'P'])\n",
    "\n",
    "# call the kernel on the card\n",
    "TILESIZE = 32\n",
    "# grid_x = ceil(max_L_V / TILESIZE)\n",
    "# L_row = np.int32(L_row)\n",
    "# grid_y = ceil(L_row / TILESIZE)\n",
    "maxthreads = 1024\n",
    "grid_x = ceil(max_L_V / maxthreads)\n",
    "grid_y = int(L_row)\n",
    "# L_row = np.int32(L_row)\n",
    "t1 = time.time()\n",
    "dot.prepared_call((grid_x, grid_y), (maxthreads, 1, 1),#(TILESIZE, TILESIZE, 1),\n",
    "                  L_row, ptr.gpudata, idx.gpudata,\n",
    "                  a_gpu.gpudata, v_gpu.gpudata, r_gpu.gpudata)\n",
    "\n",
    "print 'On GPU: {} seconds'.format(time.time()-t1)\n",
    "\n",
    "# r_gpu = gpuarray.to_gpu(r_init)\n",
    "# t1 = time.time()\n",
    "# # # reinitialized zero gpu array for the result (R = A * V)\n",
    "# # #r_gpu = gpuarray.to_gpu(r_init) #gpuarray.empty((1, ceil(L/TILE_SIZE)), np.float32)\n",
    "\n",
    "# dot.prepared_call((grid_x, 1), (BLOCKSIZE, 1, 1),\n",
    "#                   L_row,\n",
    "#                   ptr.gpudata, idx.gpudata, a_gpu.gpudata,\n",
    "#                   v_gpu.gpudata, r_gpu.gpudata)\n",
    "# print 'On GPU second prepared call: {} seconds'.format(time.time()-t1)\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Matrix A (GPU):\"\n",
    "print a_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector V (GPU):\"\n",
    "print v_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector R (GPU):\"\n",
    "print r_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector R (CPU):\"\n",
    "print r_cpu\n",
    "\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"CPU-GPU difference:\"\n",
    "print r_cpu - r_gpu.get()\n",
    "print \"L2 norm:\", la.norm(r_cpu - r_gpu.get())\n",
    "print np.allclose(r_cpu, r_gpu.get())\n",
    "\n",
    "driver.stop_profiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 648 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dot.prepared_call((grid_x, grid_y), (TILESIZE, TILESIZE, 1),\n",
    "                  L_row, ptr.gpudata, idx.gpudata,\n",
    "                  a_gpu.gpudata, v_gpu.gpudata, r_gpu.gpudata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On GPU: 0.000104904174805 seconds\n",
      "On GPU second prepared call: 9.3936920166e-05 seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Matrix A (GPU):\n",
      "[ 0.05709208 -0.03134448  0.01118304 ...,  0.0366709  -0.01318025\n",
      "  0.00872711]\n",
      "--------------------------------------------------------------------------------\n",
      "Vector V (GPU):\n",
      "[[ 0.32983508]\n",
      " [-0.80149479]\n",
      " [-1.44243596]\n",
      " ..., \n",
      " [-0.5901493 ]\n",
      " [ 0.62309832]\n",
      " [-1.80843644]]\n",
      "--------------------------------------------------------------------------------\n",
      "Vector R (GPU):\n",
      "[[-0.416356  ]\n",
      " [ 1.33046087]\n",
      " [ 0.71955558]\n",
      " ..., \n",
      " [-0.53568927]\n",
      " [-0.36451341]\n",
      " [ 0.49713843]]\n",
      "--------------------------------------------------------------------------------\n",
      "Vector R (CPU):\n",
      "[[-0.416356  ]\n",
      " [ 1.33046087]\n",
      " [ 0.71955558]\n",
      " ..., \n",
      " [-0.53568927]\n",
      " [-0.36451341]\n",
      " [ 0.49713843]]\n",
      "--------------------------------------------------------------------------------\n",
      "CPU-GPU difference:\n",
      "[[  2.77555756e-16]\n",
      " [  0.00000000e+00]\n",
      " [  3.33066907e-16]\n",
      " ..., \n",
      " [  2.22044605e-16]\n",
      " [  0.00000000e+00]\n",
      " [ -1.11022302e-16]]\n",
      "L2 norm: 4.85177441862e-14\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Multiples two square matrices together using multiple blocks and shared memory. \n",
    "Each thread block is assigned a \"tile\" of the resulting matrix and is responsible\n",
    "for generating the elements in that tile.  Each thread in a block computes one element \n",
    "of the tile.\n",
    "\"\"\"\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "from numpy import linalg as la\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '0'\n",
    "# -- initialize the device\n",
    "import pycuda.autoinit\n",
    "\n",
    "TestTranspose = False # tests the tranpose matrix product with a vector\n",
    "\n",
    "filename = '/home/mhazoglou/CUDA_Kernels/Useful_Kernels.cu'\n",
    "\n",
    "with open(filename) as fid:\n",
    "    kernel_code = fid.read()\n",
    "\n",
    "ceil = lambda x: int(math.ceil(x))\n",
    "\n",
    "# # define length of array\n",
    "# L_col = np.int32(2000)#32*32*10)#100000)#1024*1000000)\n",
    "# L_row = np.int32(200000)#1024*1+1)#13*13*13*13)\n",
    "\n",
    "\n",
    "# # create random arrays\n",
    "# a_cpu = sp.rand(L_row, L_col, density = 0.01, format = 'csr', dtype = np.float64)\n",
    "# nnz = a_cpu.nnz\n",
    "# print(\"Number of nonzero elements:\", nnz)\n",
    "# if TestTranspose:\n",
    "#     v_cpu = np.random.randn(L_row, 1).astype(np.float64)\n",
    "# else:\n",
    "#     v_cpu = np.random.randn(L_col, 1).astype(np.float64)\n",
    "\n",
    "\n",
    "# # compute reference on the CPU to verify GPU computation\n",
    "# if TestTranspose:\n",
    "#     t0 = time.time()\n",
    "#     r_cpu = (a_cpu.transpose()).dot(v_cpu)\n",
    "#     print 'On CPU: {} seconds'.format(time.time()-t0)\n",
    "# else:\n",
    "#     t0 = time.time()\n",
    "#     r_cpu = a_cpu.dot(v_cpu)\n",
    "#     print 'On CPU: {} seconds'.format(time.time()-t0)\n",
    "\n",
    "# # transfer host (CPU) memory to device (GPU) memory \n",
    "# idx = gpuarray.to_gpu(a_cpu.indices)\n",
    "# ptr = gpuarray.to_gpu(a_cpu.indptr)\n",
    "# a_gpu = gpuarray.to_gpu(a_cpu.data)\n",
    "# v_gpu = gpuarray.to_gpu(v_cpu)\n",
    "\n",
    "# # create zero initialized gpu array for the result (R = A * V)\n",
    "# if TestTranspose:\n",
    "#     r_init = np.zeros((L_col, 1), dtype = np.float64)\n",
    "# else:\n",
    "#     r_init = np.zeros((L_row, 1), dtype = np.float64)\n",
    "# r_gpu = gpuarray.to_gpu(r_init)\n",
    "\n",
    "# compile the kernel code\n",
    "mod = compiler.SourceModule(kernel_code)\n",
    "\n",
    "# get the kernel function from the compiled module\n",
    "if TestTranspose:\n",
    "    dot = mod.get_function('spmTv_csr_kernel')\n",
    "    dot.prepare([np.int32, 'P', 'P', 'P', 'P', 'P'])\n",
    "else:\n",
    "    dot = mod.get_function('spmv_csr_kernel')\n",
    "    dot.prepare([np.int32, 'P', 'P', 'P', 'P', 'P'])\n",
    "\n",
    "# call the kernel on the card\n",
    "BLOCKSIZE = 1024\n",
    "grid_x = min(ceil(L_row / BLOCKSIZE), 1024)\n",
    "t1 = time.time()\n",
    "dot.prepared_call((grid_x, 1), (BLOCKSIZE, 1, 1),\n",
    "                  L_row,\n",
    "                  ptr.gpudata, idx.gpudata, a_gpu.gpudata,\n",
    "                  v_gpu.gpudata, r_gpu.gpudata)\n",
    "\n",
    "print 'On GPU: {} seconds'.format(time.time()-t1)\n",
    "\n",
    "r_gpu = gpuarray.to_gpu(r_init)\n",
    "t1 = time.time()\n",
    "# # reinitialized zero gpu array for the result (R = A * V)\n",
    "# #r_gpu = gpuarray.to_gpu(r_init) #gpuarray.empty((1, ceil(L/TILE_SIZE)), np.float32)\n",
    "\n",
    "dot.prepared_call((grid_x, 1), (BLOCKSIZE, 1, 1),\n",
    "                  L_row,\n",
    "                  ptr.gpudata, idx.gpudata, a_gpu.gpudata,\n",
    "                  v_gpu.gpudata, r_gpu.gpudata)\n",
    "print 'On GPU second prepared call: {} seconds'.format(time.time()-t1)\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Matrix A (GPU):\"\n",
    "print a_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector V (GPU):\"\n",
    "print v_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector R (GPU):\"\n",
    "print r_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Vector R (CPU):\"\n",
    "print r_cpu\n",
    "\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"CPU-GPU difference:\"\n",
    "print r_cpu - r_gpu.get()\n",
    "print \"L2 norm:\", la.norm(r_cpu - r_gpu.get())\n",
    "print np.allclose(r_cpu, r_gpu.get())\n",
    "\n",
    "driver.stop_profiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 5.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dot.prepared_call((grid_x, 1), (BLOCKSIZE, 1, 1),\n",
    "                  L_row,\n",
    "                  ptr.gpudata, idx.gpudata, a_gpu.gpudata,\n",
    "                  v_gpu.gpudata, r_gpu.gpudata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
