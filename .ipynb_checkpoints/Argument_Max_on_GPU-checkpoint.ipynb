{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import math as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceil = lambda x: int(mt.ceil(x))\n",
    "L = np.int32(96*96) #np.int32(1024*5)\n",
    "threadsize = (1024, 1, 1)# (max(min(int(L), 1024), 32), 1, 1)\n",
    "gridsize = (min(ceil(L/threadsize[0]), 2147483647), 1)\n",
    "A = -10*np.random.rand(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lazy_arg_max(A):\n",
    "    L = A.size\n",
    "    for i in xrange(L):\n",
    "        a = A[i]\n",
    "        if i == 0:\n",
    "            max_val = a\n",
    "            arg_max = 0\n",
    "        elif a > max_val:\n",
    "            max_val = a\n",
    "            arg_max = i\n",
    "    return arg_max, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '1'\n",
    "\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_code = \"\"\"#define CUDART_NINF_D __longlong_as_double(0xfff0000000000000)\n",
    "\n",
    "// This is based on:\n",
    "//https://stackoverflow.com/questions/12626096/why-has-atomicadd-not-been-implemented-for-doubles\n",
    "// this is not thread safe the function needs to be run multiple times to converge to the right result\n",
    "// not worth the time\n",
    "__device__ void atomicArgMax(double* max_address, \n",
    "                             double max_val, \n",
    "                             uint* arg_address, \n",
    "                             uint arg_val)\n",
    "{\n",
    "    unsigned long long int* address_as_ull =\n",
    "                             (unsigned long long int*)max_address;\n",
    "    unsigned long long int old = *address_as_ull, assumed;\n",
    "    double old_max = *max_address;\n",
    "    uint old_arg = *arg_address, assumed_arg;\n",
    "    \n",
    "    do {\n",
    "        assumed = old;\n",
    "        assumed_arg = old_arg;\n",
    "        if (max_val > old_max)\n",
    "        {\n",
    "            old = atomicCAS(address_as_ull, assumed,\n",
    "                    __double_as_longlong(max_val));\n",
    "            old_arg = atomicCAS(arg_address, assumed_arg, arg_val);\n",
    "        }\n",
    "    } while ((assumed != old) and (assumed_arg != old_arg));\n",
    "}\n",
    "\n",
    "\n",
    "__inline__ __device__\n",
    "void warpReduceMax(double* val, uint* arg) \n",
    "{\n",
    "    for (unsigned int offset = warpSize/2; offset > 0; offset /= 2)\n",
    "        {\n",
    "            double tmp_val = __shfl_down(*val, offset);\n",
    "            uint   tmp_arg = __shfl_down(*arg, offset);\n",
    "            /*\n",
    "            tmp_arg can never actually be zero because of \n",
    "            __shfl_down resulting in a zero value indicates \n",
    "            a thread that is inactive (an undefined result)\n",
    "            */\n",
    "            if (tmp_val > *val)\n",
    "            {\n",
    "                *val = tmp_val;\n",
    "                *arg = tmp_arg;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "__inline__ __device__\n",
    "void blockReduceMax(double *val, \n",
    "                    uint *arg) \n",
    "{\n",
    "\n",
    "    static __shared__ double shared_val[32]; // Shared mem for 32 partial maxs\n",
    "    static __shared__ uint shared_arg[32]; // shared mem for 32 partial argmaxs\n",
    "    unsigned int lane = threadIdx.x % warpSize;\n",
    "    unsigned int wid = threadIdx.x / warpSize;\n",
    "\n",
    "    warpReduceMax(val, arg);     // Each warp performs partial reduction\n",
    "\n",
    "    if (lane==0)\n",
    "    {\n",
    "        shared_val[wid] = *val; // Write reduced value to shared memory\n",
    "        shared_arg[wid] = *arg;\n",
    "    }\n",
    "\n",
    "    __syncthreads();              // Wait for all partial reductions\n",
    "\n",
    "    // read from shared memory only if that warp existed\n",
    "    // if we have an blockDim.x which is smaller than the warpSize (32) we have a problem \n",
    "    *val = (threadIdx.x < blockDim.x / warpSize) ? shared_val[lane] : CUDART_NINF_D;\n",
    "    *arg = shared_arg[lane];\n",
    "\n",
    "\n",
    "    if (wid==0) warpReduceMax(val, arg); //Final reduce within first warp\n",
    "\n",
    "}\n",
    "\n",
    "__global__ void MaxKernel(double* A,\n",
    "                          double* max,\n",
    "                          uint* arg_max,\n",
    "                          uint L)\n",
    "{\n",
    "    uint grid_stride_arg_max;\n",
    "    double grid_stride_max, a;\n",
    "    \n",
    "    uint start = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    uint stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // a way of picking out threads that are beyond\n",
    "    // the length of the array\n",
    "    if (start < L)\n",
    "    {\n",
    "        a = A[start];\n",
    "        grid_stride_max = a;\n",
    "        grid_stride_arg_max = start;\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        a = CUDART_NINF_D;\n",
    "        grid_stride_max = CUDART_NINF_D;\n",
    "        grid_stride_arg_max = 0;\n",
    "    }\n",
    "    \n",
    "    // grid stride loop to find the max and argmax across\n",
    "    // grid strides\n",
    "    for (uint i = start;\n",
    "         i < L;\n",
    "         i += stride)\n",
    "        {\n",
    "            a = A[i];\n",
    "            if (a > grid_stride_max)\n",
    "            {\n",
    "                grid_stride_max = a;\n",
    "                grid_stride_arg_max = i;\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    // blockReduceMax gives the maximum and argmax of \n",
    "    // each block in grid_stride_max and grid_stride_arg_max\n",
    "    blockReduceMax(&grid_stride_max, &grid_stride_arg_max);\n",
    "    if (threadIdx.x == 0)\n",
    "    {\n",
    "        max[blockIdx.x] = grid_stride_max;\n",
    "        arg_max[blockIdx.x] = grid_stride_arg_max;\n",
    "    }\n",
    "    \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pycuda._driver.Function at 0x7f23ccf26b18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = compiler.SourceModule(kernel_code)\n",
    "gpu_max = mod.get_function('MaxKernel')\n",
    "gpu_max.prepare(['P', 'P', 'P', np.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A_gpu = gpuarray.to_gpu(A)\n",
    "max_gpu = gpuarray.to_gpu(A[:gridsize[0]])\n",
    "arg_max_gpu = gpuarray.to_gpu(np.array([0]*gridsize[0], dtype=np.uint32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 252 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gpu_max.prepared_call(gridsize, threadsize,\n",
    "                        A_gpu.gpudata, \n",
    "                        max_gpu.gpudata, \n",
    "                        arg_max_gpu.gpudata, \n",
    "                        L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 812, 1195, 2779, 3168, 4956, 5931, 7130, 7335, 8223], dtype=uint32),\n",
       " array([-0.00253011, -0.00483384, -0.01217089, -0.02783124, -0.00566991,\n",
       "        -0.00041875, -0.00142967, -0.00306457, -0.00379322]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_max_gpu.get(), max_gpu.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.38 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5931, -0.00041874939241015596)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lazy_arg_max(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 88 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5931, -0.00041874939241015596)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.argmax(A), np.max(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 10.22 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100000 loops, best of 3: 9.38 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.argmax(A), np.max(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fucking_code = \"\"\"\n",
    "__device__ double atomicExch(long long int* address, double val)\n",
    "{\n",
    "    /*\n",
    "    float2 split_val = *reinterpret_cast<float2*>(&val);\n",
    "    float2 old_float2 = *reinterpret_cast<float2*>(address);\n",
    "    float* address_x = &(old_float2.x);\n",
    "    float* address_y = &(old_float2.y);\n",
    "    float split_val_x = split_val.x;\n",
    "    float split_val_y = split_val.y;\n",
    "    \n",
    "    float assumed_x, assumed_y;\n",
    "    do {\n",
    "        assumed_x = old_float2.x;\n",
    "        assumed_y = old_float2.y;\n",
    "        old_float2.x = atomicExch(address_x, split_val_x);\n",
    "        old_float2.y = atomicExch(address_y, split_val_y);\n",
    "    } while ((old_float2.x != assumed_x) or (old_float2.y != assumed_y));\n",
    "    \n",
    "    return *reinterpret_cast<double*>(&old_float2);\n",
    "    */\n",
    "    unsigned long long int* address_as_ull =\n",
    "                             (unsigned long long int*)address;\n",
    "    unsigned long long int old = *address_as_ull;\n",
    "    \n",
    "    old = atomicExch(address_as_ull, \n",
    "                     __double_as_longlong(val));\n",
    "    \n",
    "    return __longlong_as_double(old);\n",
    "\n",
    "}\n",
    "\n",
    "__global__ void convert(double* A, long long int* A_ll, uint L)\n",
    "{\n",
    "    for (uint i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "         i < L;\n",
    "         i += blockDim.x * gridDim.x)\n",
    "    {\n",
    "        //A_ll[i] = __double_as_longlong(A[i]);\n",
    "        atomicExch(&(A_ll[i]), A[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = compiler.SourceModule(fucking_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pycuda._driver.Function at 0x7f23ccf1b2a8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double2ll = mod2.get_function('convert')\n",
    "double2ll.prepare(['P', 'P', np.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A_ll = gpuarray.to_gpu(np.zeros(L, dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "double2ll.prepared_call(gridsize, threadsize, A_gpu.gpudata, A_ll.gpudata, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_ll.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.02700837, -5.18968892, -7.84682252, ..., -7.05545903,\n",
       "       -4.63980326, -0.26156891])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_ll.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.02700837, -5.18968892, -7.84682252, ..., -7.05545903,\n",
       "       -4.63980326, -0.26156891])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_gpu.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
