{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import math as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ceil = lambda x: int(mt.ceil(x))\n",
    "L = np.int32(12) #np.int32(1024*5)\n",
    "threadsize = (1024, 1, 1)# (max(min(int(L), 1024), 32), 1, 1)\n",
    "gridsize = (min(ceil(L/threadsize[0]), 2147483647), 1)\n",
    "A = 10*np.random.rand(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lazy_arg_min(A):\n",
    "    L = A.size\n",
    "    for i in xrange(L):\n",
    "        a = A[i]\n",
    "        if i == 0:\n",
    "            min_val = a\n",
    "            arg_min = 0\n",
    "        elif a < min_val:\n",
    "            min_val = a\n",
    "            arg_min = i\n",
    "    return arg_min, min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '1'\n",
    "\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_code = \"\"\"#define CUDART_INF_D __longlong_as_double(0x7ff0000000000000)\n",
    "\n",
    "__inline__ __device__\n",
    "void warpReduceMin(double* val, uint* arg) \n",
    "{\n",
    "    for (unsigned int offset = warpSize/2; offset > 0; offset /= 2)\n",
    "        {\n",
    "            double tmp_val = __shfl_down(*val, offset);\n",
    "            uint   tmp_arg = __shfl_down(*arg, offset);\n",
    "            /*\n",
    "            tmp_arg can never actually be zero because of \n",
    "            __shfl_down resulting in a zero value indicates \n",
    "            a thread that is inactive (an undefined result)\n",
    "            */\n",
    "            if (tmp_val < *val)\n",
    "            {\n",
    "                *val = tmp_val;\n",
    "                *arg = tmp_arg;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "__inline__ __device__\n",
    "void blockReduceMin(double *val, \n",
    "                    uint *arg) \n",
    "{\n",
    "\n",
    "    static __shared__ double shared_val[32]; // Shared mem for 32 partial mins\n",
    "    static __shared__ uint shared_arg[32]; // shared mem for 32 partial argmins\n",
    "    unsigned int lane = threadIdx.x % warpSize;\n",
    "    unsigned int wid = threadIdx.x / warpSize;\n",
    "\n",
    "    warpReduceMin(val, arg);     // Each warp performs partial reduction\n",
    "\n",
    "    if (lane==0)\n",
    "    {\n",
    "        shared_val[wid] = *val; // Write reduced value to shared memory\n",
    "        shared_arg[wid] = *arg;\n",
    "    }\n",
    "\n",
    "    __syncthreads();              // Wait for all partial reductions\n",
    "\n",
    "    // read from shared memory only if that warp existed\n",
    "    // if we have a BlockDim.x which is smaller than the warpSize (32) we have a problem  \n",
    "    *val = (threadIdx.x < blockDim.x / warpSize) ? shared_val[lane] : CUDART_INF_D;\n",
    "    *arg = shared_arg[lane];\n",
    "\n",
    "\n",
    "    if (wid==0) warpReduceMin(val, arg); //Final reduce within first warp\n",
    "\n",
    "}\n",
    "\n",
    "__global__ void MinKernel(double* A,\n",
    "                          double* min,\n",
    "                          uint* arg_min,\n",
    "                          uint L)\n",
    "{\n",
    "    uint grid_stride_arg_min;\n",
    "    double grid_stride_min, a;\n",
    "    \n",
    "    uint start = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    uint stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // a way of picking out threads that are beyond\n",
    "    // the length of the array\n",
    "    if (start < L)\n",
    "    {\n",
    "        a = A[start];\n",
    "        grid_stride_min = a;\n",
    "        grid_stride_arg_min = start;\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        a = CUDART_INF_D;\n",
    "        grid_stride_min = CUDART_INF_D;\n",
    "        grid_stride_arg_min = 0;\n",
    "    }\n",
    "    \n",
    "    // grid stride loop to find the min and argmin across\n",
    "    // grid strides\n",
    "    for (uint i = start;\n",
    "         i < L;\n",
    "         i += stride)\n",
    "        {\n",
    "            a = A[i];\n",
    "            if (a < grid_stride_min)\n",
    "            {\n",
    "                grid_stride_min = a;\n",
    "                grid_stride_arg_min = i;\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    // blockReduceMin gives the minimum and argmin of \n",
    "    // each block in grid_stride_min and grid_stride_arg_min\n",
    "    blockReduceMin(&grid_stride_min, &grid_stride_arg_min);\n",
    "    if (threadIdx.x == 0)\n",
    "    {\n",
    "        min[blockIdx.x] = grid_stride_min;\n",
    "        arg_min[blockIdx.x] = grid_stride_arg_min;\n",
    "    }\n",
    "    \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pycuda._driver.Function at 0x7f037a6f9b18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = compiler.SourceModule(kernel_code)\n",
    "gpu_min = mod.get_function('MinKernel')\n",
    "gpu_min.prepare(['P', 'P', 'P', np.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A_gpu = gpuarray.to_gpu(A)\n",
    "min_gpu = gpuarray.to_gpu(A[:gridsize[0]])\n",
    "arg_min_gpu = gpuarray.to_gpu(np.array([0]*gridsize[0], dtype=np.uint32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 388 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gpu_min.prepared_call(gridsize, threadsize,\n",
    "                        A_gpu.gpudata, \n",
    "                        min_gpu.gpudata, \n",
    "                        arg_min_gpu.gpudata, \n",
    "                        L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11], dtype=uint32), array([ 2.2103547]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_min_gpu.get(), min_gpu.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 15 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 2.2103546957744022)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lazy_arg_min(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 34.8 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 2.2103546957744022)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.argmin(A), np.min(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
